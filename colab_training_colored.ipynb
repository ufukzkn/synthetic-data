{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15a4a2f4",
   "metadata": {},
   "source": [
    "# üõ©Ô∏è F-18 Chart Curve Extraction - Colored U-Net (RGB Output)\n",
    "\n",
    "**Renkli versiyon:** Her eƒüri farklƒ± renkte, VGG Perceptual Loss ile.\n",
    "\n",
    "**√áalƒ±≈ütƒ±rma:**\n",
    "1. Runtime > Change runtime type > **A100 GPU**\n",
    "2. T√ºm h√ºcreleri sƒ±rayla √ßalƒ±≈ütƒ±r\n",
    "\n",
    "**Colab Pro i√ßin optimize edilmi≈ütir!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6513a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU kontrol√º\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54626801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerekli k√ºt√ºphaneler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageEnhance\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "import random\n",
    "import math\n",
    "import io\n",
    "import json\n",
    "import gc\n",
    "import os\n",
    "import glob\n",
    "import psutil  # RAM monitoring i√ßin - EKLENDƒ∞!\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89283727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "# ‚ïë                    üéõÔ∏è KONFƒ∞G√úRASYON                            ‚ïë\n",
    "# ‚ïë         Bu deƒüerleri ihtiyacƒ±na g√∂re deƒüi≈ütir!                 ‚ïë\n",
    "# ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\n",
    "# === VERƒ∞ √úRETƒ∞Mƒ∞ ===\n",
    "TOTAL_IMAGES = 6000        # Fine-tune i√ßin 6000 g√ºr√ºlt√ºl√º g√∂r√ºnt√º\n",
    "IMG_SIZE = 512             # G√∂r√ºnt√º boyutu (512x512)\n",
    "\n",
    "# === Eƒûƒ∞Tƒ∞M ===\n",
    "NUM_EPOCHS = 80            # Fine-tune i√ßin 80 epoch\n",
    "BATCH_SIZE = 24            # A100: 24 optimal\n",
    "LEARNING_RATE = 5e-5       # Fine-tune i√ßin d√º≈ü√ºk LR (10x d√º≈ü√ºk)\n",
    "\n",
    "# === LOSS AƒûIRLIKLARI ===\n",
    "L1_WEIGHT = 0.5            # L1 Loss\n",
    "SSIM_WEIGHT = 0.2          # SSIM Loss\n",
    "PERCEPTUAL_WEIGHT = 0.3    # VGG Perceptual Loss\n",
    "\n",
    "# === YOLLAR (Google Colab i√ßin) ===\n",
    "# Dataset'i Drive'a kaydet (kalƒ±cƒ±!)\n",
    "SAVE_DIR = '/content/drive/MyDrive/f18_dataset_colored'  \n",
    "CHECKPOINT_DIR = '/content/drive/MyDrive/f18_checkpoints_colored'\n",
    "HISTORY_PATH = f'{CHECKPOINT_DIR}/history_colored.json'\n",
    "\n",
    "# === Dƒ∞ƒûER ===\n",
    "NUM_WORKERS = 4\n",
    "VISUALIZE_EVERY = 10\n",
    "\n",
    "# Google Drive mount\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Konfig√ºrasyon y√ºklendi!\")\n",
    "print(f\"   üìä Veri: {TOTAL_IMAGES} g√∂r√ºnt√º, {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"   üîÑ Eƒüitim: {NUM_EPOCHS} epoch, batch={BATCH_SIZE}, lr={LEARNING_RATE}\")\n",
    "print(f\"   üìâ Loss: L1={L1_WEIGHT}, SSIM={SSIM_WEIGHT}, Perceptual={PERCEPTUAL_WEIGHT}\")\n",
    "print(f\"   üíæ Dataset: {SAVE_DIR} (Drive - kalƒ±cƒ±!)\")\n",
    "print(f\"   üíæ Checkpoint: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a7c471",
   "metadata": {},
   "source": [
    "## üìä Eƒüitim Ge√ßmi≈üi Analizi & Model Kar≈üƒ±la≈ütƒ±rma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b80eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Eƒûƒ∞Tƒ∞M GE√áMƒ∞≈ûƒ∞ GRAFƒ∞ƒûƒ∞ ==========\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# History dosyasƒ±nƒ± oku\n",
    "if os.path.exists(HISTORY_PATH):\n",
    "    with open(HISTORY_PATH, 'r') as f:\n",
    "        history_raw = json.load(f)\n",
    "    \n",
    "    # Format kontrol√º\n",
    "    if isinstance(history_raw, list):\n",
    "        if history_raw and isinstance(history_raw[0], (int, float)):\n",
    "            print(\"üìÅ History formatƒ±: Sadece loss listesi\")\n",
    "            history = {'train_loss': history_raw}\n",
    "        elif history_raw and isinstance(history_raw[0], dict):\n",
    "            print(\"üìÅ History formatƒ±: Epoch bazlƒ± liste\")\n",
    "            history = {\n",
    "                'train_loss': [h.get('train_loss', h.get('loss', 0)) for h in history_raw],\n",
    "                'val_loss': [h.get('val_loss', None) for h in history_raw],\n",
    "                'lr': [h.get('lr', h.get('learning_rate', None)) for h in history_raw]\n",
    "            }\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Bilinmeyen format!\")\n",
    "            history = {'train_loss': []}\n",
    "    else:\n",
    "        print(\"üìÅ History formatƒ±: Dictionary\")\n",
    "        history = history_raw\n",
    "    \n",
    "    if not history.get('train_loss'):\n",
    "        print(\"‚ö†Ô∏è History dosyasƒ±nda train_loss bulunamadƒ±!\")\n",
    "    else:\n",
    "        train_loss = history['train_loss']\n",
    "        epochs = list(range(1, len(train_loss) + 1))\n",
    "        \n",
    "        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "        # üìä 4 PANEL GRAFƒ∞K\n",
    "        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        \n",
    "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        # 1. Loss Grafiƒüi (Normal)\n",
    "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        axes[0, 0].plot(epochs, train_loss, 'b-', label='Train Loss', linewidth=2)\n",
    "        axes[0, 0].fill_between(epochs, train_loss, alpha=0.3)\n",
    "        \n",
    "        # En iyi noktayƒ± i≈üaretle\n",
    "        best_loss = min(train_loss)\n",
    "        best_epoch = train_loss.index(best_loss) + 1\n",
    "        axes[0, 0].scatter([best_epoch], [best_loss], color='red', s=100, zorder=5, label=f'Best: {best_loss:.4f}')\n",
    "        axes[0, 0].axhline(y=best_loss, color='red', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        axes[0, 0].set_xlabel('Epoch', fontsize=11)\n",
    "        axes[0, 0].set_ylabel('Loss', fontsize=11)\n",
    "        axes[0, 0].set_title('üìâ Eƒüitim Loss Grafiƒüi', fontsize=12, fontweight='bold')\n",
    "        axes[0, 0].legend(loc='upper right')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        # 2. Loss Grafiƒüi (Logaritmik)\n",
    "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        axes[0, 1].semilogy(epochs, train_loss, 'g-', linewidth=2)\n",
    "        axes[0, 1].fill_between(epochs, train_loss, alpha=0.3, color='green')\n",
    "        axes[0, 1].scatter([best_epoch], [best_loss], color='red', s=100, zorder=5)\n",
    "        \n",
    "        axes[0, 1].set_xlabel('Epoch', fontsize=11)\n",
    "        axes[0, 1].set_ylabel('Loss (log scale)', fontsize=11)\n",
    "        axes[0, 1].set_title('üìâ Logaritmik Loss Grafiƒüi', fontsize=12, fontweight='bold')\n",
    "        axes[0, 1].grid(True, alpha=0.3, which='both')\n",
    "        \n",
    "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        # 3. Epoch Ba≈üƒ±na Loss D√º≈ü√º≈ü√º (Bar Chart)\n",
    "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        if len(train_loss) > 1:\n",
    "            loss_diff = [train_loss[i] - train_loss[i+1] for i in range(len(train_loss)-1)]\n",
    "            colors = ['green' if d > 0 else 'red' for d in loss_diff]\n",
    "            \n",
    "            axes[1, 0].bar(range(1, len(loss_diff)+1), loss_diff, color=colors, alpha=0.7)\n",
    "            axes[1, 0].axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "            \n",
    "            # Ortalama d√º≈ü√º≈ü √ßizgisi\n",
    "            avg_drop = np.mean([d for d in loss_diff if d > 0])\n",
    "            axes[1, 0].axhline(y=avg_drop, color='blue', linestyle='--', alpha=0.7, \n",
    "                              label=f'Ort. D√º≈ü√º≈ü: {avg_drop:.4f}')\n",
    "            \n",
    "            axes[1, 0].set_xlabel('Epoch', fontsize=11)\n",
    "            axes[1, 0].set_ylabel('Loss D√º≈ü√º≈ü√º', fontsize=11)\n",
    "            axes[1, 0].set_title('üìà Epoch Ba≈üƒ±na Loss Deƒüi≈üimi', fontsize=12, fontweight='bold')\n",
    "            axes[1, 0].legend(loc='upper right')\n",
    "            axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        # 4. Hareketli Ortalama (Smoothed Loss)\n",
    "        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        window_size = min(5, len(train_loss) // 4) if len(train_loss) > 10 else 3\n",
    "        if len(train_loss) >= window_size:\n",
    "            # Hareketli ortalama hesapla\n",
    "            smoothed = np.convolve(train_loss, np.ones(window_size)/window_size, mode='valid')\n",
    "            smoothed_epochs = list(range(window_size, len(train_loss) + 1))\n",
    "            \n",
    "            axes[1, 1].plot(epochs, train_loss, 'b-', alpha=0.3, linewidth=1, label='Ham Loss')\n",
    "            axes[1, 1].plot(smoothed_epochs, smoothed, 'r-', linewidth=2.5, \n",
    "                           label=f'{window_size}-Epoch Hareketli Ort.')\n",
    "            \n",
    "            # Trend √ßizgisi (lineer regresyon)\n",
    "            z = np.polyfit(epochs, train_loss, 1)\n",
    "            p = np.poly1d(z)\n",
    "            axes[1, 1].plot(epochs, p(epochs), 'g--', linewidth=1.5, alpha=0.7, label='Trend')\n",
    "            \n",
    "            axes[1, 1].set_xlabel('Epoch', fontsize=11)\n",
    "            axes[1, 1].set_ylabel('Loss', fontsize=11)\n",
    "            axes[1, 1].set_title('üìä Smoothed Loss & Trend', fontsize=12, fontweight='bold')\n",
    "            axes[1, 1].legend(loc='upper right')\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{CHECKPOINT_DIR}/training_history.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "        # üìä ƒ∞STATƒ∞STƒ∞KLER\n",
    "        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üìä Eƒûƒ∞Tƒ∞M √ñZETƒ∞\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"   üìÖ Toplam Epoch: {len(epochs)}\")\n",
    "        print(f\"   üöÄ Ba≈ülangƒ±√ß Loss: {train_loss[0]:.6f}\")\n",
    "        print(f\"   üèÜ En ƒ∞yi Loss: {best_loss:.6f} (Epoch {best_epoch})\")\n",
    "        print(f\"   üèÅ Son Loss: {train_loss[-1]:.6f}\")\n",
    "        print(f\"   üìà Toplam ƒ∞yile≈üme: {((train_loss[0] - best_loss) / train_loss[0] * 100):.1f}%\")\n",
    "        \n",
    "        # D√∂nemsel analiz\n",
    "        if len(train_loss) >= 10:\n",
    "            print(\"\\n\" + \"-\"*40)\n",
    "            print(\"üìä D√ñNEMSEL ANALƒ∞Z:\")\n",
    "            \n",
    "            # ƒ∞lk 10 epoch\n",
    "            first_10_drop = train_loss[0] - train_loss[min(9, len(train_loss)-1)]\n",
    "            print(f\"   ƒ∞lk 10 Epoch D√º≈ü√º≈ü: {first_10_drop:.6f} ({first_10_drop/train_loss[0]*100:.1f}%)\")\n",
    "            \n",
    "            # Son 10 epoch\n",
    "            last_10_drop = train_loss[-10] - train_loss[-1]\n",
    "            print(f\"   Son 10 Epoch D√º≈ü√º≈ü: {last_10_drop:.6f} ({last_10_drop/train_loss[-10]*100:.1f}%)\")\n",
    "            \n",
    "            # Convergence analizi\n",
    "            last_5_avg = np.mean(train_loss[-5:])\n",
    "            last_10_avg = np.mean(train_loss[-10:-5]) if len(train_loss) >= 10 else last_5_avg\n",
    "            convergence_rate = abs(last_5_avg - last_10_avg) / last_10_avg * 100\n",
    "            \n",
    "            print(f\"\\n   üéØ Convergence Durumu:\")\n",
    "            if convergence_rate < 2:\n",
    "                print(f\"      ‚úÖ Model CONVERGE olmu≈ü (son 5 vs √∂nceki 5: {convergence_rate:.2f}% fark)\")\n",
    "            elif convergence_rate < 5:\n",
    "                print(f\"      ‚ö†Ô∏è Model YAKIN CONVERGE (son 5 vs √∂nceki 5: {convergence_rate:.2f}% fark)\")\n",
    "            else:\n",
    "                print(f\"      üîÑ Model HALA √ñƒûRENƒ∞YOR (son 5 vs √∂nceki 5: {convergence_rate:.2f}% fark)\")\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è History dosyasƒ± bulunamadƒ±. ƒ∞lk eƒüitim mi yapƒ±yorsun?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14afa687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== üéØ MODEL TEST & KAR≈ûILA≈ûTIRMA ==========\n",
    "# S√ºr√ºm se√ßebilir veya otomatik kar≈üƒ±la≈ütƒ±rma yapabilirsin\n",
    "\n",
    "import glob\n",
    "import re\n",
    "from IPython.display import display, Image as IPImage\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# üìå AYARLAR - ƒ∞stediƒüin s√ºr√ºmleri se√ß veya None bƒ±rak (otomatik)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "SELECTED_VERSIONS = None  # None = Otomatik (best + son + -5 + -10)\n",
    "# √ñrnek manuel se√ßim: SELECTED_VERSIONS = [78, 50, 25, 10]\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "# Checkpoint dosyalarƒ±nƒ± bul ve sƒ±rala\n",
    "checkpoint_files = glob.glob(f'{CHECKPOINT_DIR}/colored_checkpoint_epoch_*.pt')\n",
    "best_model_path = f'{CHECKPOINT_DIR}/best_colored.pt'\n",
    "\n",
    "# Epoch numaralarƒ±nƒ± √ßƒ±kar\n",
    "def extract_epoch(path):\n",
    "    match = re.search(r'epoch_(\\d+)', path)\n",
    "    return int(match.group(1)) if match else 0\n",
    "\n",
    "checkpoint_dict = {extract_epoch(p): p for p in checkpoint_files}\n",
    "sorted_epochs = sorted(checkpoint_dict.keys())\n",
    "\n",
    "print(f\"üîç Bulunan checkpoint'ler: {len(sorted_epochs)}\")\n",
    "print(f\"   Epoch aralƒ±ƒüƒ±: {min(sorted_epochs)} - {max(sorted_epochs)}\")\n",
    "print(f\"   Best model: {'‚úì Var' if os.path.exists(best_model_path) else '‚úó Yok'}\")\n",
    "\n",
    "# Kar≈üƒ±la≈ütƒ±rƒ±lacak s√ºr√ºmleri belirle\n",
    "if SELECTED_VERSIONS is None:\n",
    "    # Otomatik: best + son + son-5 + son-10 + son-20\n",
    "    last_epoch = max(sorted_epochs)\n",
    "    auto_epochs = [last_epoch]\n",
    "    for offset in [5, 10, 20]:\n",
    "        target = last_epoch - offset\n",
    "        if target in sorted_epochs:\n",
    "            auto_epochs.append(target)\n",
    "        else:\n",
    "            # En yakƒ±n epoch'u bul\n",
    "            closest = min(sorted_epochs, key=lambda x: abs(x - target))\n",
    "            if closest not in auto_epochs:\n",
    "                auto_epochs.append(closest)\n",
    "    test_epochs = sorted(set(auto_epochs), reverse=True)\n",
    "    print(f\"\\nüìã Otomatik se√ßilen epoch'lar: {test_epochs}\")\n",
    "else:\n",
    "    test_epochs = [e for e in SELECTED_VERSIONS if e in sorted_epochs]\n",
    "    print(f\"\\nüìã Manuel se√ßilen epoch'lar: {test_epochs}\")\n",
    "\n",
    "# Best model'i ekle\n",
    "include_best = os.path.exists(best_model_path)\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# MODEL Y√úKLEME FONKSƒ∞YONU\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "def load_model_from_checkpoint(path, device):\n",
    "    \"\"\"Checkpoint'ten model y√ºkle (farklƒ± formatlarƒ± destekler)\"\"\"\n",
    "    ckpt = torch.load(path, map_location=device)\n",
    "    model = UNet(in_channels=3, out_channels=3)\n",
    "    \n",
    "    if isinstance(ckpt, dict):\n",
    "        if 'model_state_dict' in ckpt:\n",
    "            model.load_state_dict(ckpt['model_state_dict'])\n",
    "        elif 'state_dict' in ckpt:\n",
    "            model.load_state_dict(ckpt['state_dict'])\n",
    "        else:\n",
    "            # Doƒürudan state_dict olabilir\n",
    "            model.load_state_dict(ckpt)\n",
    "    else:\n",
    "        model.load_state_dict(ckpt)\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# TAHMƒ∞N FONKSƒ∞YONU\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "def predict_image(model, img_rgb, device):\n",
    "    \"\"\"RGB g√∂r√ºnt√ºden tahmin yap\"\"\"\n",
    "    img_resized = cv2.resize(img_rgb, (IMG_SIZE, IMG_SIZE))\n",
    "    img_tensor = torch.from_numpy(img_resized).permute(2, 0, 1).float() / 255.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred = model(img_tensor.unsqueeze(0).to(device))\n",
    "        pred = pred.squeeze(0).cpu().permute(1, 2, 0).numpy()\n",
    "        pred = (np.clip(pred, 0, 1) * 255).astype(np.uint8)\n",
    "    return pred\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# TEST G√ñR√úNT√úLERƒ∞ OLU≈ûTUR\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nüñ•Ô∏è Device: {device}\")\n",
    "\n",
    "# 3 farklƒ± test g√∂r√ºnt√ºs√º olu≈ütur\n",
    "test_samples = []\n",
    "for i, curve_type in enumerate(['peaked_oval', 'rising', 'falling']):\n",
    "    cfg = ChartConfig(\n",
    "        x_min=0.35, x_max=1.15,\n",
    "        y_min=0.05, y_max=0.16,\n",
    "        n_curves=random.randint(6, 10),\n",
    "        curve_type=curve_type,\n",
    "        curve_lw=random.uniform(0.3, 0.6),\n",
    "        add_grid=True,\n",
    "        add_arrows=True,\n",
    "        add_envelope_optimum=True,\n",
    "        add_text_boxes=True\n",
    "    )\n",
    "    img, target, _ = draw_chart_matplotlib(cfg, W=IMG_SIZE, H=IMG_SIZE)\n",
    "    img_noisy = add_scan_artifacts(img, strength=1.0)\n",
    "    test_samples.append({\n",
    "        'name': curve_type,\n",
    "        'input': img_noisy,\n",
    "        'target': target\n",
    "    })\n",
    "\n",
    "print(f\"‚úÖ {len(test_samples)} test g√∂r√ºnt√ºs√º olu≈üturuldu\")\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# MODEL KAR≈ûILA≈ûTIRMA\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# Y√ºklenecek modeller\n",
    "models_to_test = []\n",
    "if include_best:\n",
    "    models_to_test.append({'name': 'BEST', 'path': best_model_path, 'epoch': 'best'})\n",
    "for ep in test_epochs[:4]:  # Max 4 epoch\n",
    "    models_to_test.append({\n",
    "        'name': f'Epoch {ep}',\n",
    "        'path': checkpoint_dict[ep],\n",
    "        'epoch': ep\n",
    "    })\n",
    "\n",
    "print(f\"\\nüî¨ Test edilecek modeller: {[m['name'] for m in models_to_test]}\")\n",
    "\n",
    "# Her test g√∂r√ºnt√ºs√º i√ßin kar≈üƒ±la≈ütƒ±rma\n",
    "for sample in test_samples:\n",
    "    n_models = len(models_to_test)\n",
    "    fig, axes = plt.subplots(2, n_models + 2, figsize=(3.5 * (n_models + 2), 7))\n",
    "    \n",
    "    # Input ve Target\n",
    "    axes[0, 0].imshow(sample['input'])\n",
    "    axes[0, 0].set_title('üì• Input\\n(G√ºr√ºlt√ºl√º)', fontsize=10, fontweight='bold')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    axes[0, 1].imshow(sample['target'])\n",
    "    axes[0, 1].set_title('üéØ Target\\n(Ground Truth)', fontsize=10, fontweight='bold')\n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    axes[1, 0].text(0.5, 0.5, f\"Curve Type:\\n{sample['name']}\", \n",
    "                    ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].axis('off')\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    # Her model i√ßin tahmin\n",
    "    for i, m in enumerate(models_to_test):\n",
    "        try:\n",
    "            model = load_model_from_checkpoint(m['path'], device)\n",
    "            pred = predict_image(model, sample['input'], device)\n",
    "            \n",
    "            # Tahmin g√∂ster\n",
    "            axes[0, i + 2].imshow(pred)\n",
    "            axes[0, i + 2].set_title(f\"ü§ñ {m['name']}\", fontsize=10, fontweight='bold')\n",
    "            axes[0, i + 2].axis('off')\n",
    "            \n",
    "            # Fark haritasƒ±\n",
    "            diff = np.abs(pred.astype(float) - sample['target'].astype(float))\n",
    "            diff_gray = diff.mean(axis=2)\n",
    "            mae = diff_gray.mean()\n",
    "            \n",
    "            axes[1, i + 2].imshow(diff_gray, cmap='hot', vmin=0, vmax=100)\n",
    "            axes[1, i + 2].set_title(f\"üìä MAE: {mae:.1f}\", fontsize=10)\n",
    "            axes[1, i + 2].axis('off')\n",
    "            \n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        except Exception as e:\n",
    "            axes[0, i + 2].text(0.5, 0.5, f'‚ùå Hata:\\n{str(e)[:40]}', \n",
    "                               ha='center', va='center', fontsize=8, color='red')\n",
    "            axes[0, i + 2].axis('off')\n",
    "            axes[1, i + 2].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'üî¨ Model Kar≈üƒ±la≈ütƒ±rma - {sample[\"name\"].upper()}', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{CHECKPOINT_DIR}/comparison_{sample[\"name\"]}.png', dpi=120, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ KAR≈ûILA≈ûTIRMA TAMAMLANDI!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìÅ G√∂rseller kaydedildi: {CHECKPOINT_DIR}/comparison_*.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578e8568",
   "metadata": {},
   "source": [
    "## 1. Sentetik Veri √úretici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88038826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# üìä SENTETƒ∞K VERƒ∞ √úRETƒ∞Cƒ∞ - T√úM √ñZELLƒ∞KLER\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "@dataclass\n",
    "class ChartConfig:\n",
    "    x_min: float = 0.30\n",
    "    x_max: float = 1.00\n",
    "    y_min: float = 0.04\n",
    "    y_max: float = 0.15\n",
    "    n_curves: int = 8\n",
    "    curve_type: str = 'peaked'\n",
    "    curve_lw: float = 0.6\n",
    "    add_grid: bool = True\n",
    "    add_arrows: bool = True\n",
    "    add_envelope_optimum: bool = True\n",
    "    add_envelope_endurance: bool = False\n",
    "    add_vmax_line: bool = False\n",
    "    add_text_boxes: bool = True\n",
    "    add_fuel_labels: bool = True\n",
    "    add_drag_labels: bool = True\n",
    "\n",
    "\n",
    "def generate_curve_shape(x, curve_type, curve_index, total_curves):\n",
    "    alt = curve_index / max(total_curves - 1, 1)\n",
    "    x_norm = (x - x.min()) / (x.max() - x.min() + 1e-8)\n",
    "    \n",
    "    if curve_type == 'peaked':\n",
    "        peak_pos = 0.30 + random.uniform(-0.05, 0.05)\n",
    "        start_y = 0.12 + random.uniform(-0.02, 0.02)\n",
    "        peak_y = 0.45 + alt * 0.40 + random.uniform(-0.03, 0.03)\n",
    "        end_y = 0.20 + alt * 0.25 + random.uniform(-0.02, 0.02)\n",
    "        y = np.zeros_like(x_norm)\n",
    "        for i, t in enumerate(x_norm):\n",
    "            if t <= peak_pos:\n",
    "                progress = t / peak_pos\n",
    "                y[i] = start_y + (peak_y - start_y) * (1 - (1 - progress) ** 2)\n",
    "            else:\n",
    "                progress = (t - peak_pos) / (1 - peak_pos)\n",
    "                y[i] = peak_y - (peak_y - end_y) * (progress ** 0.7)\n",
    "                \n",
    "    elif curve_type == 'peaked_oval':\n",
    "        peak_pos = 0.45 + random.uniform(-0.07, 0.07)\n",
    "        start_y = 0.12 + random.uniform(-0.02, 0.02)\n",
    "        peak_y = 0.45 + alt * 0.38 + random.uniform(-0.03, 0.03)\n",
    "        end_y = 0.20 + alt * 0.25 + random.uniform(-0.02, 0.02)\n",
    "        y = np.zeros_like(x_norm)\n",
    "        for i, t in enumerate(x_norm):\n",
    "            if t <= peak_pos:\n",
    "                progress = t / peak_pos\n",
    "                y[i] = start_y + (peak_y - start_y) * (math.sin(progress * math.pi / 2) ** 1.2)\n",
    "            else:\n",
    "                progress = (t - peak_pos) / (1 - peak_pos)\n",
    "                y[i] = end_y + (peak_y - end_y) * (math.cos(progress * math.pi / 2) ** 1.2)\n",
    "                \n",
    "    elif curve_type == 'wavy':\n",
    "        freq = random.choice([1.0, 1.5, 2.0])\n",
    "        phase = random.uniform(0, 1)\n",
    "        wave = 0.5 + 0.25 * np.sin(2 * np.pi * (x_norm * freq + phase))\n",
    "        wave += 0.12 * np.sin(4 * np.pi * (x_norm * freq + phase))\n",
    "        hump_center = random.uniform(0.55, 0.70)\n",
    "        hump = 0.12 * np.exp(-((x_norm - hump_center) / 0.22) ** 2)\n",
    "        y = wave + hump\n",
    "        y = np.clip(y, 0.05, 0.95)\n",
    "        \n",
    "    elif curve_type == 'rising':\n",
    "        start_y = 0.08 + alt * 0.05 + random.uniform(-0.02, 0.02)\n",
    "        end_y = 0.55 + alt * 0.30 + random.uniform(-0.03, 0.03)\n",
    "        curvature = random.uniform(0.7, 1.3)\n",
    "        y = start_y + (end_y - start_y) * (x_norm ** curvature)\n",
    "        \n",
    "    elif curve_type == 'falling':\n",
    "        start_y = 0.65 + alt * 0.25 + random.uniform(-0.03, 0.03)\n",
    "        end_y = 0.12 + alt * 0.10 + random.uniform(-0.02, 0.02)\n",
    "        curvature = random.uniform(0.5, 1.0)\n",
    "        y = start_y - (start_y - end_y) * (x_norm ** curvature)\n",
    "        \n",
    "    else:\n",
    "        return generate_curve_shape(x, random.choice(['peaked', 'peaked_oval', 'rising', 'falling', 'wavy']),\n",
    "                                   curve_index, total_curves)\n",
    "    return y\n",
    "\n",
    "\n",
    "def fig_to_array(fig, dpi=150, tight=True):\n",
    "    buf = io.BytesIO()\n",
    "    if tight:\n",
    "        fig.savefig(buf, format='png', dpi=dpi, bbox_inches='tight', pad_inches=0.02,\n",
    "                    facecolor='white', edgecolor='none')\n",
    "    else:\n",
    "        fig.savefig(buf, format='png', dpi=dpi, \n",
    "                    facecolor=fig.get_facecolor(), edgecolor='none')\n",
    "    plt.close(fig)\n",
    "    buf.seek(0)\n",
    "    img = Image.open(buf).convert('RGB')\n",
    "    return np.array(img)\n",
    "\n",
    "\n",
    "def draw_chart_matplotlib(config, W=512, H=512):\n",
    "    \"\"\"Draw chart and return image + colored mask (RGB) - FULL VERSION.\"\"\"\n",
    "    fig_w, fig_h = W / 100, H / 100\n",
    "    \n",
    "    x = np.linspace(config.x_min + 0.02, config.x_max - 0.02, 400)\n",
    "    curves_data = []\n",
    "    \n",
    "    for i in range(config.n_curves):\n",
    "        y_norm = generate_curve_shape(x, config.curve_type, i, config.n_curves)\n",
    "        y = config.y_min + y_norm * (config.y_max - config.y_min)\n",
    "        y = np.clip(y, config.y_min + 0.001, config.y_max - 0.001)\n",
    "        curves_data.append((x.copy(), y))\n",
    "    \n",
    "    # ========== FULL IMAGE ==========\n",
    "    fig1, ax1 = plt.subplots(figsize=(fig_w, fig_h))\n",
    "    ax1.set_xlim(config.x_min, config.x_max)\n",
    "    ax1.set_ylim(config.y_min, config.y_max)\n",
    "    \n",
    "    # Grid - Minor grid dahil\n",
    "    if config.add_grid:\n",
    "        x_range = config.x_max - config.x_min\n",
    "        y_range = config.y_max - config.y_min\n",
    "        x_major = 0.1 if x_range > 0.5 else 0.05\n",
    "        y_major = 0.01 if y_range < 0.08 else 0.02\n",
    "        ax1.set_xticks(np.arange(config.x_min, config.x_max + 0.001, x_major))\n",
    "        ax1.set_xticks(np.arange(config.x_min, config.x_max + 0.001, x_major/2), minor=True)\n",
    "        ax1.set_yticks(np.arange(config.y_min, config.y_max + 0.001, y_major))\n",
    "        ax1.set_yticks(np.arange(config.y_min, config.y_max + 0.001, y_major/2), minor=True)\n",
    "        ax1.grid(True, which='major', linewidth=0.8, alpha=0.5, color='black')\n",
    "        ax1.grid(True, which='minor', linewidth=0.4, alpha=0.3, color='black')\n",
    "    \n",
    "    ax1.axhline(y=config.y_min, color='black', linewidth=2.0, zorder=10)\n",
    "    ax1.axvline(x=config.x_min, color='black', linewidth=2.0, zorder=10)\n",
    "    ax1.tick_params(axis='both', which='major', length=6, width=1.5, direction='in')\n",
    "    ax1.tick_params(axis='both', which='minor', length=3, width=1.0, direction='in')\n",
    "    \n",
    "    for spine in ax1.spines.values():\n",
    "        spine.set_linewidth(1.5)\n",
    "    \n",
    "    ax1.set_xlabel('MACH NUMBER', fontsize=10, fontweight='bold')\n",
    "    ax1.set_ylabel('SPECIFIC RANGE ‚Äî NAUTICAL MILES PER POUND OF FUEL', fontsize=8)\n",
    "    \n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê ANA EƒûRƒ∞LER ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    for cx, cy in curves_data:\n",
    "        ax1.plot(cx, cy, 'k-', linewidth=config.curve_lw)\n",
    "    \n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê OPTIMUM CRUISE ENVELOPE ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    if config.add_envelope_optimum:\n",
    "        if config.curve_type in ['peaked', 'peaked_oval']:\n",
    "            envelope_pts = [(cx[np.argmax(cy)], cy.max()) for cx, cy in curves_data]\n",
    "        else:\n",
    "            envelope_pts = [(cx[int(len(cx)*0.5)], cy[int(len(cy)*0.5)]) for cx, cy in curves_data]\n",
    "        envelope_pts.sort(key=lambda p: p[1])\n",
    "        ex, ey = zip(*envelope_pts)\n",
    "        ax1.plot(ex, ey, 'k-', linewidth=1.2)\n",
    "        ax1.text(ex[0] - 0.03, ey[-1] + (config.y_max - config.y_min) * 0.02,\n",
    "                'OPTIMUM\\nCRUISE', fontsize=8, ha='right', va='bottom')\n",
    "    \n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê MAXIMUM ENDURANCE ENVELOPE ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    if config.add_envelope_endurance:\n",
    "        envelope_pts = [(cx[int(len(cx)*0.2)], cy[int(len(cy)*0.2)]) for cx, cy in curves_data]\n",
    "        envelope_pts.sort(key=lambda p: p[1])\n",
    "        ex, ey = zip(*envelope_pts)\n",
    "        ax1.plot(ex, ey, 'k-', linewidth=1.2)\n",
    "        ax1.text(ex[-1] - 0.02, ey[0] - (config.y_max - config.y_min) * 0.02,\n",
    "                'MAXIMUM\\nENDURANCE', fontsize=8, ha='right', va='top')\n",
    "    \n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê ARROWS & FUEL LABELS ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    if config.add_arrows:\n",
    "        fuel_flows = ['3000', '3500', '4000', '4500', '5000', '5500', '6000', '6500', '7000', '7500', '8000', '8500']\n",
    "        for idx, (cx, cy) in enumerate(reversed(curves_data)):\n",
    "            if idx >= len(fuel_flows):\n",
    "                break\n",
    "            \n",
    "            # Arrow pozisyonu: %50 saƒü u√ßta, %50 ortada\n",
    "            if random.random() < 0.5:\n",
    "                arrow_idx = -1\n",
    "                x_head = cx[arrow_idx]\n",
    "                y_head = cy[arrow_idx]\n",
    "                dx = random.uniform(0.04, 0.08)\n",
    "                dy = random.uniform(-0.005, 0.005)\n",
    "                x_tail = x_head + dx\n",
    "                y_tail = y_head + dy\n",
    "            else:\n",
    "                mid_start = len(cx) // 4\n",
    "                mid_end = 3 * len(cx) // 4\n",
    "                arrow_idx = random.randint(mid_start, mid_end)\n",
    "                x_head = cx[arrow_idx]\n",
    "                y_head = cy[arrow_idx]\n",
    "                angle = random.uniform(20, 70)\n",
    "                dist = random.uniform(0.05, 0.10)\n",
    "                if random.random() < 0.5:\n",
    "                    dx = dist * math.cos(math.radians(angle))\n",
    "                    dy = dist * math.sin(math.radians(angle))\n",
    "                else:\n",
    "                    dx = dist * math.cos(math.radians(-angle))\n",
    "                    dy = dist * math.sin(math.radians(-angle))\n",
    "                x_tail = x_head + dx\n",
    "                y_tail = y_head + dy\n",
    "            \n",
    "            # Arrow √ßizgisi\n",
    "            ax1.plot([x_tail, x_head], [y_tail, y_head], color=\"black\", linewidth=0.6)\n",
    "            \n",
    "            # Arrow ba≈üƒ±\n",
    "            if random.random() < 0.4:\n",
    "                arrow_style = random.choice([\"-|>\", \"->\"])\n",
    "                fill_style = \"none\"\n",
    "            else:\n",
    "                arrow_style = random.choice([\"-|>\", \"-|>\", \"->\"])\n",
    "                fill_style = \"black\"\n",
    "            \n",
    "            ax1.annotate(\n",
    "                \"\",\n",
    "                xy=(x_head, y_head),\n",
    "                xytext=(x_tail, y_tail),\n",
    "                arrowprops=dict(\n",
    "                    arrowstyle=arrow_style,\n",
    "                    lw=random.uniform(0.7, 1.1),\n",
    "                    color=\"black\",\n",
    "                    fc=fill_style,\n",
    "                    shrinkA=0, shrinkB=0,\n",
    "                    mutation_scale=random.uniform(12, 18),\n",
    "                ),\n",
    "            )\n",
    "            \n",
    "            # Fuel flow label\n",
    "            if config.add_fuel_labels:\n",
    "                label_x = x_tail + random.uniform(0.02, 0.05)\n",
    "                ax1.text(label_x, y_tail + random.uniform(-0.002, 0.002),\n",
    "                        fuel_flows[idx], fontsize=8, va='center', ha='left')\n",
    "    \n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê KESƒ∞KLƒ∞ √áƒ∞ZGƒ∞LER (DASHED LINES) ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    # Her zaman ekle - ger√ßek F-18 grafiklerinde √ßok yaygƒ±n\n",
    "    n_dashed = random.randint(3, 8)\n",
    "    for _ in range(n_dashed):\n",
    "        dcx = config.x_min + (config.x_max - config.x_min) * random.uniform(0.15, 0.85)\n",
    "        dcy = config.y_min + (config.y_max - config.y_min) * random.uniform(0.15, 0.85)\n",
    "        dash_len = random.uniform(0.04, 0.12)\n",
    "        dash_angle = math.radians(random.choice([30, 40, 45, 50, 60, 70]))\n",
    "        dash_dx = dash_len * math.cos(dash_angle)\n",
    "        dash_dy = dash_len * math.sin(dash_angle)\n",
    "        # Farklƒ± dash stilleri\n",
    "        dash_style = random.choice([(0, (12, 6)), (0, (8, 4)), (0, (15, 8)), (0, (6, 3))])\n",
    "        ax1.plot([dcx, dcx + dash_dx], [dcy, dcy + dash_dy],\n",
    "                color=\"black\", linewidth=random.uniform(0.4, 0.7), linestyle=dash_style)\n",
    "    \n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê TEXT BOXES & LEGEND ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    if config.add_text_boxes:\n",
    "        # Fuel flow kutusu (saƒü √ºst)\n",
    "        ax1.text(config.x_max - 0.05, config.y_max - 0.005,\n",
    "                'TOTAL FUEL FLOW‚Äî\\nPOUNDS PER HOUR',\n",
    "                fontsize=8, ha='right', va='top',\n",
    "                bbox=dict(boxstyle='square,pad=0.3', facecolor='white', edgecolor='black'))\n",
    "        \n",
    "        # Legend box (sol √ºst)\n",
    "        legend_x = config.x_min + (config.x_max - config.x_min) * 0.12\n",
    "        legend_y = config.y_max - (config.y_max - config.y_min) * 0.08\n",
    "        ax1.text(legend_x, legend_y,\n",
    "                '‚óÑ‚îÄ CRUISE    DASH ‚îÄ‚ñ∫\\n      AOA          AOA\\n(USED FOR INTERFERENCE\\n DRAG DETERMINATION)',\n",
    "                fontsize=7, ha='left', va='top',\n",
    "                bbox=dict(boxstyle='square,pad=0.3', facecolor='white', edgecolor='black'))\n",
    "    \n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê DRAG INDEX LABELS ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    if config.add_drag_labels:\n",
    "        labels = ['0.00', '25.00', '50.00', '75.00', '100.00', '125.00', '150.00']\n",
    "        base_x = config.x_min + (config.x_max - config.x_min) * 0.65\n",
    "        base_y = config.y_min + (config.y_max - config.y_min) * 0.15\n",
    "        for i, lbl in enumerate(labels[:random.randint(4, 7)]):\n",
    "            ax1.text(base_x + random.uniform(-0.02, 0.02),\n",
    "                    base_y + i * (config.y_max - config.y_min) * 0.05,\n",
    "                    lbl, fontsize=7, alpha=0.9)\n",
    "    \n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê VMAX LINE ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    if config.add_vmax_line:\n",
    "        vmax_pts = [(cx[int(len(cx)*0.85)], cy[int(len(cy)*0.85)]) for cx, cy in curves_data]\n",
    "        vmax_pts.sort(key=lambda p: p[1])\n",
    "        vx, vy = zip(*vmax_pts)\n",
    "        ax1.plot(vx, vy, 'k--', linewidth=0.8)\n",
    "        ax1.text(vx[-1], vy[-1] + 0.003, r'$V_{max}$(MIL)', fontsize=7)\n",
    "    \n",
    "    full_img = fig_to_array(fig1, dpi=150, tight=True)\n",
    "    full_img = cv2.resize(full_img, (W, H))\n",
    "    \n",
    "    # ========== COLORED MASK (RGB, HSV renk sistemi ile) ==========\n",
    "    fig2, ax2 = plt.subplots(figsize=(fig_w, fig_h))\n",
    "    ax2.set_xlim(config.x_min, config.x_max)\n",
    "    ax2.set_ylim(config.y_min, config.y_max)\n",
    "    ax2.set_position([0, 0, 1, 1])\n",
    "    ax2.axis('off')\n",
    "    fig2.patch.set_facecolor('black')\n",
    "    ax2.set_facecolor('black')\n",
    "    \n",
    "    n_curves = len(curves_data)\n",
    "    for i, (cx, cy) in enumerate(curves_data):\n",
    "        hue = int(180 * i / max(n_curves, 1))\n",
    "        hsv_color = np.array([[[hue, 255, 255]]], dtype=np.uint8)\n",
    "        bgr_color = cv2.cvtColor(hsv_color, cv2.COLOR_HSV2BGR)[0, 0]\n",
    "        rgb_color = (int(bgr_color[2]), int(bgr_color[1]), int(bgr_color[0]))\n",
    "        ax2.plot(cx, cy, color=np.array(rgb_color) / 255.0, linewidth=config.curve_lw + 0.2, zorder=2)\n",
    "    \n",
    "    colored_img = fig_to_array(fig2, dpi=150, tight=False)\n",
    "    colored_img = cv2.resize(colored_img, (W, H))\n",
    "    \n",
    "    return full_img, colored_img, curves_data\n",
    "\n",
    "\n",
    "def random_config():\n",
    "    \"\"\"Rastgele grafik konfig√ºrasyonu √ºret\"\"\"\n",
    "    x_ranges = [\n",
    "        (0.30, 0.95), (0.30, 1.00), (0.40, 1.10), (0.50, 1.20),\n",
    "        (0.50, 1.30), (0.50, 1.40), (0.60, 1.40)\n",
    "    ]\n",
    "    y_ranges = [\n",
    "        (0.04, 0.15), (0.05, 0.15), (0.06, 0.17), (0.07, 0.18),\n",
    "        (0.08, 0.19), (0.08, 0.20), (0.05, 0.14)\n",
    "    ]\n",
    "    \n",
    "    x_min, x_max = random.choice(x_ranges)\n",
    "    y_min, y_max = random.choice(y_ranges)\n",
    "    \n",
    "    curve_types = ['peaked_oval'] * 28 + ['peaked'] * 26 + ['rising'] * 16 + ['falling'] * 14 + ['wavy'] * 10 + ['mixed'] * 6\n",
    "    curve_type = random.choice(curve_types)\n",
    "    \n",
    "    # Curve type'a g√∂re eƒüri sayƒ±sƒ±\n",
    "    if curve_type == 'wavy':\n",
    "        n_curves = random.randint(3, 5)  # 3-5 arasƒ± (d√º≈ü√ºr√ºld√º)\n",
    "    elif curve_type in ['falling', 'mixed']:\n",
    "        n_curves = random.randint(4, 7)\n",
    "    elif curve_type == 'rising':\n",
    "        n_curves = random.randint(5, 8)\n",
    "    else:\n",
    "        n_curves = random.randint(6, 12)\n",
    "    \n",
    "    return ChartConfig(\n",
    "        x_min=x_min, x_max=x_max,\n",
    "        y_min=y_min, y_max=y_max,\n",
    "        n_curves=n_curves,\n",
    "        curve_type=curve_type,\n",
    "        curve_lw=random.uniform(0.3, 0.6),\n",
    "        add_grid=random.random() < 0.95,\n",
    "        add_arrows=random.random() < 0.85,\n",
    "        add_envelope_optimum=random.random() < 0.70,\n",
    "        add_envelope_endurance=random.random() < 0.35,\n",
    "        add_vmax_line=random.random() < 0.25,\n",
    "        add_text_boxes=random.random() < 0.75,\n",
    "        add_fuel_labels=random.random() < 0.80,\n",
    "        add_drag_labels=random.random() < 0.55,\n",
    "    )\n",
    "\n",
    "\n",
    "def add_scan_artifacts(img, strength=1.0):\n",
    "    \"\"\"Scan/photocopy artifacts ekle\"\"\"\n",
    "    pil_img = Image.fromarray(img)\n",
    "    angle = random.uniform(-1.2, 1.2) * strength\n",
    "    pil_img = pil_img.rotate(angle, fillcolor=(255, 255, 255), resample=Image.BICUBIC)\n",
    "    pil_img = ImageEnhance.Brightness(pil_img).enhance(random.uniform(0.90, 1.10))\n",
    "    pil_img = ImageEnhance.Contrast(pil_img).enhance(random.uniform(0.88, 1.12))\n",
    "    arr = np.array(pil_img).astype(np.float32) / 255.0\n",
    "    noise = np.random.normal(0, 0.012 * strength, arr.shape)\n",
    "    arr = np.clip(arr + noise, 0, 1)\n",
    "    buf = io.BytesIO()\n",
    "    Image.fromarray((arr * 255).astype(np.uint8)).save(buf, format='JPEG', quality=random.randint(50, 80))\n",
    "    buf.seek(0)\n",
    "    return np.array(Image.open(buf).convert('RGB'))\n",
    "\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# üîç T√úM CURVE TYPE'LARDAN √ñRNEK G√ñSTER\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "print(\"üîç Her Curve Type'tan √ñrnek Grafik √úretiliyor...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "curve_types_all = ['peaked', 'peaked_oval', 'rising', 'falling', 'wavy']\n",
    "fig, axes = plt.subplots(len(curve_types_all), 2, figsize=(14, 5 * len(curve_types_all)))\n",
    "\n",
    "for row, ctype in enumerate(curve_types_all):\n",
    "    # Bu tip i√ßin config olu≈ütur (t√ºm √∂zellikler a√ßƒ±k)\n",
    "    cfg = ChartConfig(\n",
    "        x_min=0.35, x_max=1.15,\n",
    "        y_min=0.05, y_max=0.16,\n",
    "        n_curves=4 if ctype == 'wavy' else random.randint(6, 9),\n",
    "        curve_type=ctype,\n",
    "        curve_lw=random.uniform(0.4, 0.55),\n",
    "        add_grid=True,\n",
    "        add_arrows=True,\n",
    "        add_envelope_optimum=True,\n",
    "        add_envelope_endurance=True,\n",
    "        add_vmax_line=True,\n",
    "        add_text_boxes=True,\n",
    "        add_fuel_labels=True,\n",
    "        add_drag_labels=True\n",
    "    )\n",
    "    \n",
    "    img, colored, _ = draw_chart_matplotlib(cfg, W=512, H=512)\n",
    "    img_noisy = add_scan_artifacts(img, strength=1.0)\n",
    "    \n",
    "    # Input g√∂ster\n",
    "    axes[row, 0].imshow(img_noisy)\n",
    "    axes[row, 0].set_title(f'üì• INPUT: {ctype.upper()}\\n({cfg.n_curves} eƒüri, g√ºr√ºlt√ºl√º)', fontsize=11, fontweight='bold')\n",
    "    axes[row, 0].axis('off')\n",
    "    \n",
    "    # Target g√∂ster\n",
    "    axes[row, 1].imshow(colored)\n",
    "    axes[row, 1].set_title(f'üéØ TARGET: RGB Mask\\n(HSV renklendirme)', fontsize=11, fontweight='bold')\n",
    "    axes[row, 1].axis('off')\n",
    "\n",
    "plt.suptitle('üìä T√úM CURVE TYPE\\'LARI - √ñrnek Grafikler\\n(Grid, Dashed Lines, Arrows, Legend Box, Vmax Line)', \n",
    "             fontsize=14, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/all_curve_types_sample.png', dpi=120, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Kontrol Listesi:\")\n",
    "print(\"   ‚úì Grid (major + minor √ßizgiler)\")\n",
    "print(\"   ‚úì Kesikli √ßizgiler (dashed lines)\")\n",
    "print(\"   ‚úì Oklar ve fuel flow etiketleri\")\n",
    "print(\"   ‚úì Legend box (sol √ºst)\")\n",
    "print(\"   ‚úì Text box (saƒü √ºst)\")\n",
    "print(\"   ‚úì Vmax line (kesikli)\")\n",
    "print(\"   ‚úì Envelope √ßizgileri\")\n",
    "print(\"   ‚úì HSV renklendirme (mask'ta)\")\n",
    "print(\"   ‚úì Scan artifacts (g√ºr√ºlt√º, rotation, JPEG)\")\n",
    "print(f\"\\nüìä Wavy eƒüri sayƒ±sƒ±: 3-5 arasƒ± (d√º≈ü√ºr√ºld√º)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97b3d7b",
   "metadata": {},
   "source": [
    "## 2. Veri √úretimi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b7b053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# üìä VERƒ∞ √úRETƒ∞Mƒ∞ - SESSION BASED (HIZLI!)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# ‚ö° Veriler /content klas√∂r√ºne yazƒ±lƒ±r (hƒ±zlƒ±)\n",
    "# üíæ Eƒüitim bitince ayrƒ± cell ile Drive'a kaydedebilirsin\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "import time\n",
    "\n",
    "# Session-based klas√∂r (Colab RAM'de, √ßok hƒ±zlƒ±!)\n",
    "SESSION_DATA_DIR = '/content/session_dataset'\n",
    "os.makedirs(f'{SESSION_DATA_DIR}/images', exist_ok=True)\n",
    "os.makedirs(f'{SESSION_DATA_DIR}/colored', exist_ok=True)\n",
    "\n",
    "existing = len(glob.glob(f'{SESSION_DATA_DIR}/images/*.png'))\n",
    "print(f\"üìÅ Dataset konumu: {SESSION_DATA_DIR} (Session - hƒ±zlƒ±!)\")\n",
    "print(f\"üìä Mevcut g√∂r√ºnt√º sayƒ±sƒ±: {existing}\")\n",
    "\n",
    "# Eƒüer yeterli veri varsa atla\n",
    "if existing >= TOTAL_IMAGES:\n",
    "    print(f\"‚úÖ Dataset zaten mevcut ({existing} g√∂r√ºnt√º). √úretim atlanƒ±yor.\")\n",
    "else:\n",
    "    start_idx = existing if existing > 0 else 0\n",
    "    \n",
    "    print(f\"\\nüöÄ {TOTAL_IMAGES - start_idx} g√∂r√ºnt√º √ºretiliyor...\")\n",
    "    print(\"   ‚ö° Session storage kullanƒ±lƒ±yor (√ßok hƒ±zlƒ±!)\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i in tqdm(range(start_idx, TOTAL_IMAGES), initial=start_idx, total=TOTAL_IMAGES, \n",
    "                  desc=\"Veri √úretimi\", unit=\"img\"):\n",
    "        config = random_config()\n",
    "        img, colored, _ = draw_chart_matplotlib(config, W=IMG_SIZE, H=IMG_SIZE)\n",
    "        \n",
    "        # G√ºr√ºlt√º ekle\n",
    "        img = add_scan_artifacts(img, strength=random.uniform(0.8, 1.5))\n",
    "        \n",
    "        # Session klas√∂r√ºne kaydet (hƒ±zlƒ±!)\n",
    "        cv2.imwrite(f'{SESSION_DATA_DIR}/images/{i:05d}.png', cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
    "        cv2.imwrite(f'{SESSION_DATA_DIR}/colored/{i:05d}.png', cv2.cvtColor(colored, cv2.COLOR_RGB2BGR))\n",
    "        \n",
    "        plt.close('all')\n",
    "        \n",
    "        # Her 100 g√∂r√ºnt√ºde garbage collect\n",
    "        if i % 100 == 0:\n",
    "            gc.collect()\n",
    "        \n",
    "        # Her 1000 g√∂r√ºnt√ºde progress raporu\n",
    "        if i > 0 and i % 1000 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            rate = (i - start_idx + 1) / elapsed\n",
    "            remaining = (TOTAL_IMAGES - i) / rate\n",
    "            print(f\"   ‚è±Ô∏è {i}/{TOTAL_IMAGES} - {rate:.1f} img/s - Kalan: {remaining/60:.1f} dk\")\n",
    "    \n",
    "    elapsed_total = time.time() - start_time\n",
    "    print(f\"\\n‚úÖ Veri √ºretimi tamamlandƒ±!\")\n",
    "    print(f\"   üìä Toplam: {TOTAL_IMAGES} g√∂r√ºnt√º\")\n",
    "    print(f\"   ‚è±Ô∏è S√ºre: {elapsed_total/60:.1f} dakika\")\n",
    "    print(f\"   üíæ Konum: {SESSION_DATA_DIR}\")\n",
    "    print(f\"\\n‚ö†Ô∏è NOT: Bu veriler session'da. Kalƒ±cƒ± istersen a≈üaƒüƒ±daki cell'i √ßalƒ±≈ütƒ±r!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddcc1ee",
   "metadata": {},
   "source": [
    "## 3. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9480b7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# üì¶ DATASET CLASS\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "class ColoredDataset(Dataset):\n",
    "    def __init__(self, data_dir, img_size=512, augment=True):\n",
    "        self.data_dir = data_dir\n",
    "        self.img_size = img_size\n",
    "        self.augment = augment\n",
    "        self.image_files = sorted(os.listdir(f'{data_dir}/images'))\n",
    "        print(f\"ColoredDataset: Found {len(self.image_files)} images in {data_dir}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(f'{self.data_dir}/images/{self.image_files[idx]}')\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        colored = cv2.imread(f'{self.data_dir}/colored/{self.image_files[idx]}')\n",
    "        colored = cv2.cvtColor(colored, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        img = cv2.resize(img, (self.img_size, self.img_size))\n",
    "        colored = cv2.resize(colored, (self.img_size, self.img_size))\n",
    "        \n",
    "        if self.augment:\n",
    "            if random.random() > 0.5:\n",
    "                img = np.fliplr(img).copy()\n",
    "                colored = np.fliplr(colored).copy()\n",
    "            if random.random() > 0.5:\n",
    "                factor = random.uniform(0.9, 1.1)\n",
    "                img = np.clip(img * factor, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        img = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0\n",
    "        colored = torch.from_numpy(colored).permute(2, 0, 1).float() / 255.0\n",
    "        \n",
    "        return img, colored\n",
    "\n",
    "\n",
    "# Session data kullan (hƒ±zlƒ±!)\n",
    "dataset = ColoredDataset(SESSION_DATA_DIR, img_size=IMG_SIZE, augment=True)\n",
    "img, colored = dataset[0]\n",
    "print(f\"‚úÖ Dataset hazƒ±r!\")\n",
    "print(f\"   Image shape: {img.shape}\")\n",
    "print(f\"   Colored shape: {colored.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ede7524",
   "metadata": {},
   "source": [
    "## 4. U-Net Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a51dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, features=[64, 128, 256, 512]):\n",
    "        super().__init__()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        for f in features:\n",
    "            self.downs.append(DoubleConv(in_channels, f))\n",
    "            in_channels = f\n",
    "        \n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1] * 2)\n",
    "        \n",
    "        for f in reversed(features):\n",
    "            self.ups.append(nn.ConvTranspose2d(f * 2, f, 2, 2))\n",
    "            self.ups.append(DoubleConv(f * 2, f))\n",
    "        \n",
    "        self.final = nn.Conv2d(features[0], out_channels, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "        \n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "        \n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections = skip_connections[::-1]\n",
    "        \n",
    "        for i in range(0, len(self.ups), 2):\n",
    "            x = self.ups[i](x)\n",
    "            skip = skip_connections[i // 2]\n",
    "            if x.shape != skip.shape:\n",
    "                x = F.interpolate(x, size=skip.shape[2:], mode='bilinear', align_corners=True)\n",
    "            x = torch.cat([skip, x], dim=1)\n",
    "            x = self.ups[i + 1](x)\n",
    "        \n",
    "        return torch.sigmoid(self.final(x))\n",
    "\n",
    "\n",
    "model = UNet(out_channels=3)  # RGB output!\n",
    "x = torch.randn(1, 3, 512, 512)\n",
    "y = model(x)\n",
    "print(f\"Input: {x.shape} -> Output: {y.shape}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d16afa1",
   "metadata": {},
   "source": [
    "## 5. Loss Functions (L1 + SSIM + Perceptual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28548cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSIMLoss(nn.Module):\n",
    "    def __init__(self, window_size=11):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.channel = 3\n",
    "        self.window = self._create_window(window_size, self.channel)\n",
    "    \n",
    "    def _create_window(self, window_size, channel):\n",
    "        def gaussian(window_size, sigma):\n",
    "            x = torch.arange(window_size).float() - window_size // 2\n",
    "            gauss = torch.exp(-x.pow(2) / (2 * sigma ** 2))\n",
    "            return gauss / gauss.sum()\n",
    "        \n",
    "        _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
    "        _2D_window = _1D_window.mm(_1D_window.t()).unsqueeze(0).unsqueeze(0)\n",
    "        window = _2D_window.expand(channel, 1, window_size, window_size).contiguous()\n",
    "        return window\n",
    "    \n",
    "    def forward(self, img1, img2):\n",
    "        channel = img1.size(1)\n",
    "        window = self.window.to(img1.device).type_as(img1)\n",
    "        \n",
    "        mu1 = F.conv2d(img1, window, padding=self.window_size//2, groups=channel)\n",
    "        mu2 = F.conv2d(img2, window, padding=self.window_size//2, groups=channel)\n",
    "        \n",
    "        mu1_sq = mu1.pow(2)\n",
    "        mu2_sq = mu2.pow(2)\n",
    "        mu1_mu2 = mu1 * mu2\n",
    "        \n",
    "        sigma1_sq = F.conv2d(img1*img1, window, padding=self.window_size//2, groups=channel) - mu1_sq\n",
    "        sigma2_sq = F.conv2d(img2*img2, window, padding=self.window_size//2, groups=channel) - mu2_sq\n",
    "        sigma12 = F.conv2d(img1*img2, window, padding=self.window_size//2, groups=channel) - mu1_mu2\n",
    "        \n",
    "        C1 = 0.01 ** 2\n",
    "        C2 = 0.03 ** 2\n",
    "        \n",
    "        ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2)) / ((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))\n",
    "        return 1 - ssim_map.mean()\n",
    "\n",
    "\n",
    "class PerceptualLoss(nn.Module):\n",
    "    \"\"\"VGG-based perceptual loss.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        vgg = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
    "        self.features = nn.Sequential(*list(vgg.features)[:16]).eval()\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
    "        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        pred = (pred - self.mean) / self.std\n",
    "        target = (target - self.mean) / self.std\n",
    "        pred_features = self.features(pred)\n",
    "        target_features = self.features(target)\n",
    "        return F.l1_loss(pred_features, target_features)\n",
    "\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, l1_weight=0.5, ssim_weight=0.2, perceptual_weight=0.3):\n",
    "        super().__init__()\n",
    "        self.l1_weight = l1_weight\n",
    "        self.ssim_weight = ssim_weight\n",
    "        self.perceptual_weight = perceptual_weight\n",
    "        self.l1 = nn.L1Loss()\n",
    "        self.ssim = SSIMLoss()\n",
    "        self.perceptual = PerceptualLoss()\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        l1_loss = self.l1(pred, target)\n",
    "        ssim_loss = self.ssim(pred, target)\n",
    "        perceptual_loss = self.perceptual(pred, target)\n",
    "        return self.l1_weight * l1_loss + self.ssim_weight * ssim_loss + self.perceptual_weight * perceptual_loss\n",
    "\n",
    "\n",
    "criterion = CombinedLoss(l1_weight=L1_WEIGHT, ssim_weight=SSIM_WEIGHT, perceptual_weight=PERCEPTUAL_WEIGHT)\n",
    "print(f\"‚úÖ Loss: L1({L1_WEIGHT}) + SSIM({SSIM_WEIGHT}) + Perceptual({PERCEPTUAL_WEIGHT})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c17f52c",
   "metadata": {},
   "source": [
    "## 6. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b05839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# ‚öôÔ∏è Eƒûƒ∞Tƒ∞M SETUP\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üñ•Ô∏è Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Model\n",
    "model = UNet(out_channels=3).to(device)\n",
    "\n",
    "# Loss\n",
    "criterion = CombinedLoss(\n",
    "    l1_weight=L1_WEIGHT, \n",
    "    ssim_weight=SSIM_WEIGHT, \n",
    "    perceptual_weight=PERCEPTUAL_WEIGHT\n",
    ").to(device)\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# CHECKPOINT Y√úKLEME (varsa devam et)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "START_EPOCH = 0\n",
    "best_loss = float('inf')\n",
    "history = {'train_loss': [], 'lr': []}\n",
    "\n",
    "# Session checkpoint (eƒüitim sƒ±rasƒ±nda kullanƒ±lacak)\n",
    "SESSION_CHECKPOINT_DIR = '/content/checkpoints'\n",
    "os.makedirs(SESSION_CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Drive'da √∂nceden eƒüitilmi≈ü model var mƒ±?\n",
    "best_model_path = f'{CHECKPOINT_DIR}/best_colored.pt'\n",
    "if os.path.exists(best_model_path):\n",
    "    print(\"\\nüìÇ √ñnceki best model bulundu, y√ºkleniyor...\")\n",
    "    try:\n",
    "        checkpoint = torch.load(best_model_path, map_location=device)\n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            best_loss = checkpoint.get('loss', checkpoint.get('best_loss', float('inf')))\n",
    "            prev_epoch = checkpoint.get('epoch', 0)\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint)\n",
    "            best_loss = float('inf')\n",
    "            prev_epoch = 0\n",
    "        print(f\"   ‚úÖ Model y√ºklendi!\")\n",
    "        print(f\"   üìä √ñnceki best loss: {best_loss:.6f}\")\n",
    "        print(f\"   üìä √ñnceki epoch: {prev_epoch}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Y√ºkleme hatasƒ±: {e}\")\n",
    "        print(\"   üÜï Sƒ±fƒ±rdan ba≈ülanƒ±yor...\")\n",
    "else:\n",
    "    print(\"\\nüÜï ƒ∞lk eƒüitim - sƒ±fƒ±rdan ba≈ülƒ±yor...\")\n",
    "\n",
    "# Optimizer ve Scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=20, T_mult=2, eta_min=1e-6)\n",
    "\n",
    "# DataLoader - Session data kullan!\n",
    "train_dataset = ColoredDataset(SESSION_DATA_DIR, img_size=IMG_SIZE, augment=True)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nüìã Eƒüitim Ayarlarƒ±:\")\n",
    "print(f\"   Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"   Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"   Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"   Dataset: {len(train_dataset)} g√∂r√ºnt√º\")\n",
    "print(f\"   Batches/Epoch: {len(train_loader)}\")\n",
    "print(f\"\\nüí° Checkpoint'ler session'da tutulacak ({SESSION_CHECKPOINT_DIR})\")\n",
    "print(f\"   Eƒüitim bitince Drive'a kaydetmek i√ßin ayrƒ± cell √ßalƒ±≈ütƒ±r!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9ffb6d",
   "metadata": {},
   "source": [
    "## 7. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a57617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, targets in tqdm(loader, desc=\"Training\"):\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def visualize_predictions(model, dataset, device, n=3):\n",
    "    model.eval()\n",
    "    fig, axes = plt.subplots(n, 3, figsize=(15, 5*n))\n",
    "    if n == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    indices = random.sample(range(len(dataset)), n)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, idx in enumerate(indices):\n",
    "            img, target = dataset[idx]\n",
    "            pred = model(img.unsqueeze(0).to(device)).cpu().squeeze()\n",
    "            \n",
    "            axes[i, 0].imshow(img.permute(1, 2, 0))\n",
    "            axes[i, 0].set_title('Input')\n",
    "            axes[i, 0].axis('off')\n",
    "            \n",
    "            axes[i, 1].imshow(target.permute(1, 2, 0))\n",
    "            axes[i, 1].set_title('Ground Truth')\n",
    "            axes[i, 1].axis('off')\n",
    "            \n",
    "            axes[i, 2].imshow(pred.permute(1, 2, 0))\n",
    "            axes[i, 2].set_title('Prediction')\n",
    "            axes[i, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/content/predictions.png', dpi=100)\n",
    "    plt.close()\n",
    "    display(IPImage('/content/predictions.png'))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af512da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# üöÄ Eƒûƒ∞Tƒ∞M LOOP - SESSION BASED (HIZLI!)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# Checkpoint'ler session'da tutulur ‚Üí Eƒüitim hƒ±zlƒ±!\n",
    "# Eƒüitim bitince Drive'a kaydetmek i√ßin a≈üaƒüƒ±daki ayrƒ± cell'i √ßalƒ±≈ütƒ±r\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "# Session history\n",
    "session_history = {'train_loss': [], 'lr': [], 'best_loss': best_loss}\n",
    "\n",
    "print(f\"\\nüöÄ Eƒüitim ba≈ülƒ±yor...\")\n",
    "print(f\"   Best loss: {best_loss:.6f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "no_improve_count = 0\n",
    "EARLY_STOP_PATIENCE = 15\n",
    "\n",
    "for epoch in range(START_EPOCH, NUM_EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Train\n",
    "    loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    scheduler.step()\n",
    "    \n",
    "    # History g√ºncelle\n",
    "    session_history['train_loss'].append(loss)\n",
    "    session_history['lr'].append(scheduler.get_last_lr()[0])\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    remaining = (NUM_EPOCHS - epoch - 1) * epoch_time\n",
    "    \n",
    "    # Resource kullanƒ±mƒ±\n",
    "    ram_percent = psutil.virtual_memory().percent\n",
    "    gpu_mem = torch.cuda.memory_allocated() / 1e9 if torch.cuda.is_available() else 0\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:3d}/{NUM_EPOCHS} | Loss: {loss:.6f} | LR: {scheduler.get_last_lr()[0]:.2e} | \"\n",
    "          f\"‚è±Ô∏è {epoch_time:.0f}s | RAM: {ram_percent:.0f}% | GPU: {gpu_mem:.1f}GB\")\n",
    "    \n",
    "    # Best model kontrol√º\n",
    "    if loss < best_loss:\n",
    "        improvement = (best_loss - loss) / best_loss * 100\n",
    "        best_loss = loss\n",
    "        session_history['best_loss'] = best_loss\n",
    "        no_improve_count = 0\n",
    "        \n",
    "        # Session'a best model kaydet (hƒ±zlƒ±!)\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            'best_loss': best_loss,\n",
    "        }, f'{SESSION_CHECKPOINT_DIR}/best_colored.pt')\n",
    "        print(f\"   üèÜ YENƒ∞ BEST! Loss: {loss:.6f} (‚Üì{improvement:.2f}%)\")\n",
    "    else:\n",
    "        no_improve_count += 1\n",
    "        if no_improve_count >= EARLY_STOP_PATIENCE:\n",
    "            print(f\"\\n‚ö†Ô∏è {EARLY_STOP_PATIENCE} epoch iyile≈üme yok. Early stopping!\")\n",
    "            break\n",
    "    \n",
    "    # Her 5 epoch'ta checkpoint (session'a)\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'loss': loss,\n",
    "        }, f'{SESSION_CHECKPOINT_DIR}/checkpoint_epoch_{epoch+1}.pt')\n",
    "    \n",
    "    # G√∂rselle≈ütirme\n",
    "    if (epoch + 1) % VISUALIZE_EVERY == 0:\n",
    "        print(f\"\\nüìä G√∂rselle≈ütirme (Epoch {epoch+1}):\")\n",
    "        visualize_predictions(model, train_dataset, device, n=2)\n",
    "        print(f\"   Kalan s√ºre: ~{remaining/60:.1f} dk\")\n",
    "        print(\"=\" * 60)\n",
    "    \n",
    "    # Memory temizle\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Eƒüitim bitti\n",
    "total_time = time.time() - start_time\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"‚úÖ Eƒûƒ∞Tƒ∞M TAMAMLANDI!\")\n",
    "print(f\"   üìä En iyi loss: {best_loss:.6f}\")\n",
    "print(f\"   ‚è±Ô∏è Toplam s√ºre: {total_time/60:.1f} dakika\")\n",
    "print(f\"   üíæ Session checkpoint: {SESSION_CHECKPOINT_DIR}\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n‚ö†Ô∏è CHECKPOINT'LER SESSION'DA!\")\n",
    "print(f\"   Kalƒ±cƒ± kaydetmek i√ßin a≈üaƒüƒ±daki 'Drive'a Kaydet' cell'ini √ßalƒ±≈ütƒ±r!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20883ca",
   "metadata": {},
   "source": [
    "## 8. Loss Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a3df35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Fƒ∞NAL SONU√áLAR ==========\n",
    "if os.path.exists(HISTORY_PATH):\n",
    "    with open(HISTORY_PATH, 'r') as f:\n",
    "        final_history = json.load(f)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Loss grafiƒüi\n",
    "    epochs = range(1, len(final_history['train_loss']) + 1)\n",
    "    axes[0].plot(epochs, final_history['train_loss'], 'b-', linewidth=2, label='Loss')\n",
    "    axes[0].axhline(y=final_history.get('best_loss', min(final_history['train_loss'])), \n",
    "                    color='r', linestyle='--', label=f'Best: {final_history.get(\"best_loss\", min(final_history[\"train_loss\"])):.6f}')\n",
    "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0].set_ylabel('Loss', fontsize=12)\n",
    "    axes[0].set_title('üìâ Fine-Tune Loss Grafiƒüi', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # LR grafiƒüi\n",
    "    if 'lr' in final_history:\n",
    "        axes[1].plot(epochs, final_history['lr'], 'g-', linewidth=2)\n",
    "        axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "        axes[1].set_ylabel('Learning Rate', fontsize=12)\n",
    "        axes[1].set_title('üìà Learning Rate', fontsize=14, fontweight='bold')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        axes[1].set_yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{CHECKPOINT_DIR}/finetune_results.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüìä Fƒ∞NAL SONU√áLAR:\")\n",
    "    print(f\"   Toplam Epoch: {len(final_history['train_loss'])}\")\n",
    "    print(f\"   En ƒ∞yi Loss: {final_history.get('best_loss', min(final_history['train_loss'])):.6f}\")\n",
    "    print(f\"   Son Loss: {final_history['train_loss'][-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a340bdc2",
   "metadata": {},
   "source": [
    "## 9. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e34e33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_show(model, image, device):\n",
    "    model.eval()\n",
    "    if isinstance(image, str):\n",
    "        img = cv2.imread(image)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    else:\n",
    "        img = image\n",
    "    \n",
    "    orig_h, orig_w = img.shape[:2]\n",
    "    img_resized = cv2.resize(img, (512, 512))\n",
    "    img_tensor = torch.from_numpy(img_resized).permute(2, 0, 1).float() / 255.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred = model(img_tensor.unsqueeze(0).to(device)).cpu().squeeze()\n",
    "    \n",
    "    pred_img = (pred.permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n",
    "    pred_full = cv2.resize(pred_img, (orig_w, orig_h))\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title('Input')\n",
    "    axes[0].axis('off')\n",
    "    axes[1].imshow(pred_full)\n",
    "    axes[1].set_title('Prediction (Colored Curves)')\n",
    "    axes[1].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/content/test_result.png', dpi=100)\n",
    "    plt.close()\n",
    "    display(IPImage('/content/test_result.png'))\n",
    "    return pred_full\n",
    "\n",
    "\n",
    "test_config = random_config()\n",
    "test_img, _, _ = draw_chart_matplotlib(test_config, W=800, H=600)\n",
    "test_img = add_scan_artifacts(test_img)\n",
    "_ = predict_and_show(model, test_img, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4839c29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# üíæ DRIVE'A KAYDET - Eƒûƒ∞Tƒ∞M Bƒ∞TTƒ∞KTEN SONRA √áALI≈ûTIR!\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# Bu cell'i eƒüitim tamamlandƒ±ktan sonra √ßalƒ±≈ütƒ±r.\n",
    "# Session'daki checkpoint'leri Drive'a kopyalar.\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "import shutil\n",
    "\n",
    "print(\"üíæ Session checkpoint'leri Drive'a kopyalanƒ±yor...\")\n",
    "\n",
    "# Session'daki t√ºm checkpoint'leri Drive'a kopyala\n",
    "session_checkpoints = glob.glob(f'{SESSION_CHECKPOINT_DIR}/*.pt')\n",
    "\n",
    "if not session_checkpoints:\n",
    "    print(\"‚ö†Ô∏è Session'da checkpoint bulunamadƒ±!\")\n",
    "else:\n",
    "    for src_path in session_checkpoints:\n",
    "        filename = os.path.basename(src_path)\n",
    "        dst_path = f'{CHECKPOINT_DIR}/{filename}'\n",
    "        shutil.copy2(src_path, dst_path)\n",
    "        print(f\"   ‚úÖ {filename} ‚Üí Drive\")\n",
    "    \n",
    "    # History'i de kaydet\n",
    "    if 'session_history' in dir() and session_history.get('train_loss'):\n",
    "        with open(HISTORY_PATH, 'w') as f:\n",
    "            json.dump(session_history, f, indent=2)\n",
    "        print(f\"   ‚úÖ history_colored.json ‚Üí Drive\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ T√ºm checkpoint'ler Drive'a kaydedildi!\")\n",
    "    print(f\"   üìÅ Konum: {CHECKPOINT_DIR}\")\n",
    "    print(f\"   üìä Best loss: {best_loss:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
